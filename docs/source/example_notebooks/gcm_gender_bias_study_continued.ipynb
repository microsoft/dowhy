{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Bias Study continued: what-if analysis\n",
    "\n",
    "This is a continuation of the Gender Bias Study where we explore this dataset and the Causality library a bit further. Specifially, we'll explore what the overall outcome of admissions would look like if female and male preferences were equal. And to do that, we'll use the library's **intervention** feature. So let's load the data and fit the graph just like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd, networkx as nx, dowhy.gcm as gcm\n",
    "gcm.config.disable_progress_bars()\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"student_admissions_berkeley.csv\")\n",
    "\n",
    "causal_model = gcm.StructuralCausalModel(\n",
    "    nx.DiGraph([(\"Gender\", \"Department\"), (\"Gender\", \"Admission\"), (\"Department\", \"Admission\")]))\n",
    "gcm.auto.assign_causal_mechanisms(causal_model, data)\n",
    "\n",
    "gcm.util.plot(causal_model.graph)\n",
    "\n",
    "gcm.fit(causal_model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said the imbalance comes from *different preferences* in department choice between males and females and that departments have different admission rates. We can show the different admission rates not just in the original data, we can also use Causality's `draw_sample` function to sample generated data from the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = gcm.draw_samples(causal_model, num_samples=10000)\n",
    "sampled_data.groupby([\"Admission\", \"Department\"]).size().unstack().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this resembles the different admission rates that we already saw in the real data.\n",
    "\n",
    "So let’s verify **what** happens **if** we could make department preference equal between men and women. We can treat this as an intervention and compute the scenario like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "interventional_data = gcm.interventional_samples(causal_model, \n",
    "                                                 {'Department': lambda d: random.choice(['A', 'B', 'C', 'D', 'E', 'F'])}, \n",
    "                                                 num_samples_to_draw=100000)\n",
    "\n",
    "admissions = interventional_data.groupby([\"Admission\", \"Gender\"]).size().unstack()\n",
    "admissions.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the admission rates now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Male admission rate:\", round(100*admissions[\"Male\"][\"Yes\"]/(admissions[\"Male\"][\"Yes\"]+admissions[\"Male\"][\"No\"])))\n",
    "print(\"Female admission rate:\", round(100*admissions[\"Female\"][\"Yes\"]/(admissions[\"Female\"][\"Yes\"]+admissions[\"Female\"][\"No\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like in fact *women* were preferred. But is this true? Let's take a step back. This represents only a single snapshot over the data. What we actually need to do, is to repeat this computation many times, and over a random subset of our data. Why is that?\n",
    "\n",
    "We trained our causal graph on one dataset, drew one set of samples from the interventional distribution, and obtained a point estimate from those samples. There can be variations both during the training process as well as during the drawing process. To account for that variability, we can train the same causal graph on different random subsets of our original data. So now, performing interventions on those trained causal graphs and repeating the whole operation many times, keeps our results more faithful to the data. We now have a proper confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def intervene(causal_model, interventions):\n",
    "    admissions = gcm.interventional_samples(causal_model, \n",
    "                                           interventions, \n",
    "                                           num_samples_to_draw=10000).groupby([\"Admission\", \"Gender\"]).size().unstack()\n",
    "    return {\n",
    "        'male': admissions[\"Male\"][\"Yes\"]/(admissions[\"Male\"][\"Yes\"]+admissions[\"Male\"][\"No\"]), \n",
    "        'female': admissions[\"Female\"][\"Yes\"]/(admissions[\"Female\"][\"Yes\"]+admissions[\"Female\"][\"No\"])\n",
    "    }\n",
    "\n",
    "median, intervals = gcm.confidence_intervals(\n",
    "    gcm.bootstrap_training_and_sampling(intervene, \n",
    "                                        causal_model, data, \n",
    "                                        interventions={'Department': lambda d: random.choice(['A', 'B', 'C', 'D', 'E', 'F'])}), \n",
    "    confidence_level=0.95, \n",
    "    num_bootstrap_resamples=50)\n",
    "\n",
    "median, intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the `intervals`, we can see that we have a big overlap. However, the results still suggest that there was in fact a small bias towards women. The original [UC Berkeley gender bias study](https://homepage.stat.uiowa.edu/~mbognar/1030/Bickel-Berkeley.pdf) comes to the conclusion that there is a “[small but statistically significant bias in favor of women](https://en.wikipedia.org/wiki/Simpson%27s_paradox#UC_Berkeley_gender_bias)“, which would match our results here. However, we should keep in mind, there are always hidden confounders similar to Department, that are not contained in this dataset. "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
